{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4xx0hWSSRUX"
      },
      "source": [
        "**Downloading quickdraw libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J35bG4BbKblx"
      },
      "source": [
        "# **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "8p7E1neyKaST"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from quickdraw import QuickDrawDataGroup\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch._C import device\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktDEEjb1KjpX"
      },
      "source": [
        "**Set the device**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "6Sw7c-a92dJ8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y0aiWlyKxKX"
      },
      "source": [
        "**Defining the categories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "4NDhxi0nLNnv"
      },
      "outputs": [],
      "source": [
        "animals = (\"bat\", \"bee\", \"cat\", \"duck\", \"elephant\", \"lion\", \"octopus\", \"rabbit\", \"snail\", \"whale\")\n",
        "image_size = (32, 32)\n",
        "max_drawings = 3000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFo_o1NfLcy5"
      },
      "source": [
        "**Making the directories for each animal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_animal(animal, image_size, max_drawings, recognized):\n",
        "    \n",
        "    directory = Path(\"data/\" + animal)\n",
        "    if not directory.exists():\n",
        "        directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    imgs = QuickDrawDataGroup(animal, max_drawings=max_drawings, recognized=recognized)\n",
        "    for img in imgs.drawings:\n",
        "        filename = directory.as_posix() + \"/\" + str(img.key_id) + \".png\"\n",
        "        img.get_image(stroke_width=3).resize(image_size).save(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTdT7HC6LZzS",
        "outputId": "100bfca8-6a16-4875-898e-90450a47c63a"
      },
      "outputs": [],
      "source": [
        "# uncomment this to download the images!\n",
        "\n",
        "# for animal in animals:\n",
        "#     download_animal(animal, image_size, max_drawings, recognized=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p52o8vHSMsgy"
      },
      "source": [
        "# **Defining dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdC32U8RMzJg",
        "outputId": "b78518cb-a780-4a31-fb5d-8138185cb75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 30000 files belonging to 10 classes.\n",
            "Using 24000 files for training.\n",
            "Found 30000 files belonging to 10 classes.\n",
            "Using 6000 files for validation.\n"
          ]
        }
      ],
      "source": [
        "# create dataloaders\n",
        "training_dataloader = image_dataset_from_directory(\n",
        "    directory=\"data\", \n",
        "    subset=\"training\",\n",
        "    color_mode=\"grayscale\",\n",
        "    validation_split=0.2, \n",
        "    seed=42,\n",
        "    batch_size=32,\n",
        "    image_size=image_size)\n",
        "\n",
        "validation_dataloader = image_dataset_from_directory(\n",
        "    directory=\"data\",\n",
        "    subset=\"validation\",\n",
        "    color_mode=\"grayscale\",\n",
        "    validation_split=0.2,\n",
        "    seed=42,\n",
        "    batch_size=32,\n",
        "    image_size=image_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMOVqaRHUUk"
      },
      "source": [
        "# **Creating the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "# design model\n",
        "model = tf.keras.Sequential([\n",
        "    Rescaling(1./255, input_shape=(32, 32, 1)), BatchNormalization(),\n",
        "\n",
        "    Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzWmFA-XHzUB"
      },
      "source": [
        "# **Compiling the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=1e-3)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=1e-3),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAmWY6u1IXXr"
      },
      "source": [
        "# **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "750/750 [==============================] - 61s 82ms/step - loss: 2.0504 - accuracy: 0.3084 - val_loss: 1.7332 - val_accuracy: 0.4443\n",
            "Epoch 2/25\n",
            "750/750 [==============================] - 71s 94ms/step - loss: 1.6139 - accuracy: 0.4725 - val_loss: 1.4143 - val_accuracy: 0.5603\n",
            "Epoch 3/25\n",
            "750/750 [==============================] - 63s 84ms/step - loss: 1.3811 - accuracy: 0.5570 - val_loss: 1.2228 - val_accuracy: 0.6192\n",
            "Epoch 4/25\n",
            "750/750 [==============================] - 60s 80ms/step - loss: 1.2282 - accuracy: 0.6050 - val_loss: 1.1091 - val_accuracy: 0.6563\n",
            "Epoch 5/25\n",
            "750/750 [==============================] - 61s 82ms/step - loss: 1.1096 - accuracy: 0.6504 - val_loss: 0.9996 - val_accuracy: 0.6930\n",
            "Epoch 6/25\n",
            "750/750 [==============================] - 60s 81ms/step - loss: 0.9953 - accuracy: 0.6883 - val_loss: 0.9010 - val_accuracy: 0.7260\n",
            "Epoch 7/25\n",
            "750/750 [==============================] - 62s 82ms/step - loss: 0.8982 - accuracy: 0.7188 - val_loss: 0.8319 - val_accuracy: 0.7408\n",
            "Epoch 8/25\n",
            "750/750 [==============================] - 62s 82ms/step - loss: 0.8121 - accuracy: 0.7451 - val_loss: 0.7650 - val_accuracy: 0.7618\n",
            "Epoch 9/25\n",
            "750/750 [==============================] - 60s 80ms/step - loss: 0.7457 - accuracy: 0.7677 - val_loss: 0.7264 - val_accuracy: 0.7707\n",
            "Epoch 10/25\n",
            "750/750 [==============================] - 64s 86ms/step - loss: 0.6891 - accuracy: 0.7850 - val_loss: 0.6970 - val_accuracy: 0.7812\n",
            "Epoch 11/25\n",
            "750/750 [==============================] - 74s 98ms/step - loss: 0.6425 - accuracy: 0.7984 - val_loss: 0.6810 - val_accuracy: 0.7875\n",
            "Epoch 12/25\n",
            "750/750 [==============================] - 60s 80ms/step - loss: 0.5979 - accuracy: 0.8120 - val_loss: 0.6232 - val_accuracy: 0.8058\n",
            "Epoch 13/25\n",
            "750/750 [==============================] - 61s 82ms/step - loss: 0.5586 - accuracy: 0.8234 - val_loss: 0.6139 - val_accuracy: 0.8023\n",
            "Epoch 14/25\n",
            "750/750 [==============================] - 60s 80ms/step - loss: 0.5206 - accuracy: 0.8352 - val_loss: 0.5956 - val_accuracy: 0.8108\n",
            "Epoch 15/25\n",
            "750/750 [==============================] - 61s 81ms/step - loss: 0.4896 - accuracy: 0.8470 - val_loss: 0.5751 - val_accuracy: 0.8183\n",
            "Epoch 16/25\n",
            "750/750 [==============================] - 60s 80ms/step - loss: 0.4627 - accuracy: 0.8553 - val_loss: 0.5735 - val_accuracy: 0.8182\n",
            "Epoch 17/25\n",
            "750/750 [==============================] - 59s 79ms/step - loss: 0.4376 - accuracy: 0.8599 - val_loss: 0.5693 - val_accuracy: 0.8180\n",
            "Epoch 18/25\n",
            "750/750 [==============================] - 65s 87ms/step - loss: 0.4175 - accuracy: 0.8652 - val_loss: 0.5557 - val_accuracy: 0.8262\n",
            "Epoch 19/25\n",
            "750/750 [==============================] - 59s 79ms/step - loss: 0.3881 - accuracy: 0.8755 - val_loss: 0.5503 - val_accuracy: 0.8270\n",
            "Epoch 20/25\n",
            "750/750 [==============================] - 59s 79ms/step - loss: 0.3685 - accuracy: 0.8810 - val_loss: 0.5446 - val_accuracy: 0.8283\n",
            "Epoch 21/25\n",
            "750/750 [==============================] - 61s 81ms/step - loss: 0.3500 - accuracy: 0.8888 - val_loss: 0.5424 - val_accuracy: 0.8323\n",
            "Epoch 22/25\n",
            "750/750 [==============================] - 57s 76ms/step - loss: 0.3297 - accuracy: 0.8922 - val_loss: 0.5433 - val_accuracy: 0.8288\n",
            "Epoch 23/25\n",
            "750/750 [==============================] - 57s 76ms/step - loss: 0.3052 - accuracy: 0.9027 - val_loss: 0.5408 - val_accuracy: 0.8338\n",
            "Epoch 24/25\n",
            "750/750 [==============================] - 61s 82ms/step - loss: 0.2934 - accuracy: 0.9054 - val_loss: 0.5494 - val_accuracy: 0.8353\n",
            "Epoch 25/25\n",
            "750/750 [==============================] - 60s 80ms/step - loss: 0.2765 - accuracy: 0.9108 - val_loss: 0.5418 - val_accuracy: 0.8355\n",
            "INFO:tensorflow:Assets written to: ./models/final_model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./models/final_model/assets\n"
          ]
        }
      ],
      "source": [
        "epochs = 25\n",
        "\n",
        "model.fit(training_dataloader, validation_data = validation_dataloader, epochs = epochs)\n",
        "\n",
        "model.save(\"./models/final_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daonDlfJIpNb"
      },
      "source": [
        "# **Testing accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "188/188 - 4s - loss: 0.5418 - accuracy: 0.8355 - 4s/epoch - 19ms/step\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(validation_dataloader, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKpRw0MnJQNo"
      },
      "source": [
        "**Input a random image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load in model\n",
        "loaded_model = tf.keras.models.load_model(\"./models/test_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n",
            "This doodle is of a ~ bat ~!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for images, labels in training_dataloader.take(1):  # only take first element of dataset\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "  \n",
        "image_data = numpy_images[0].astype(\"uint8\")\n",
        "label_data = numpy_labels[0]\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"label: \" + training_dataloader.class_names[label_data])\n",
        "plt.imshow(image_data, cmap='gray', vmin=0, vmax=255)\n",
        "\n",
        "reshaped_image = np.resize(image_data,(1, 32, 32, 1))\n",
        "test_predictions = loaded_model.predict(reshaped_image)\n",
        "result_index = np.argmax(test_predictions)\n",
        "result_label = training_dataloader.class_names[result_index]\n",
        "print(f\"This doodle is of a ~ {result_label} ~!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
