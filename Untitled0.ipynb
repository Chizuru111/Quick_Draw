{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libraries**"
      ],
      "metadata": {
        "id": "J35bG4BbKblx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from quickdraw import QuickDrawDataGroup\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "8p7E1neyKaST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change the device to GPU**"
      ],
      "metadata": {
        "id": "ktDEEjb1KjpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch._C import device\n",
        "# Get cpu or gpu device (if available) for training.\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sw7c-a92dJ8",
        "outputId": "cd72dab5-efeb-43a5-ed85-6969bb2cc783"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "directory_path = '/content/Dataset'\n",
        "file_paths = glob.glob(directory_path + '/*.npy')\n",
        "\n",
        "# Check the shape of each file\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = np.load(f, allow_pickle=True)\n",
        "        print(f\"{file_path}: {data.shape}\")\n"
      ],
      "metadata": {
        "id": "tNo85ew25ndb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creat"
      ],
      "metadata": {
        "id": "30BjBbe3MDq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the categories**"
      ],
      "metadata": {
        "id": "-Y0aiWlyKxKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = (\"bat\", \"bee\", \"cat\", \"duck\", \"elephant\", \"lion\", \"octopus\", \"rabbit\", \"snail\", \"whale\")\n",
        "image_size = (64, 64)\n",
        "max_drawings = 3000"
      ],
      "metadata": {
        "id": "4NDhxi0nLNnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the doodles**"
      ],
      "metadata": {
        "id": "LFo_o1NfLcy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (64, 64)\n",
        "categories = [\"airplane\", \"apple\", \"bicycle\", \"car\", \"cat\", \"cloud\", \"dog\", \"hamburger\", \"fish\", \"flower\", \"banana\", \"bird\", \"eye\", \"fork\", \"hat\"]\n",
        "max_drawings = 5000\n",
        "\n",
        "def generate_class_images(name, max_drawings, recognized):\n",
        "    directory = Path(\"data/\" + name)\n",
        "\n",
        "    if not directory.exists():\n",
        "        directory.mkdir(parents=True)\n",
        "\n",
        "    images = QuickDrawDataGroup(name, max_drawings=max_drawings, recognized=recognized)\n",
        "    for img in images.drawings:\n",
        "        filename = directory.as_posix() + \"/\" + str(img.key_id) + \".png\"\n",
        "        img.get_image(stroke_width=3).resize(image_size).save(filename)\n",
        "\n",
        "for label in categories:\n",
        "    generate_class_images(label, max_drawings=max_drawings, recognized=True)"
      ],
      "metadata": {
        "id": "wTdT7HC6LZzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining dataset**"
      ],
      "metadata": {
        "id": "p52o8vHSMsgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    \"data\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    \"data\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "NdC32U8RMzJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(X_train, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(X_test, batch_size=batch_size)\n",
        "\n",
        "# for X, y in test_dataloader:\n",
        "#     print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "#     print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "#     break"
      ],
      "metadata": {
        "id": "jpEld-Vz1RPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating the model**"
      ],
      "metadata": {
        "id": "NNMOVqaRHUUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "def softmax(x):\n",
        "  return torch.exp(x) / torch.sum(torch.exp(x), dim=0)\n",
        "\n",
        "loss_fn = tf.keras.metrics.categorical_crossentropy\n",
        "lr = 5e-1\n",
        "optm = SGD(learning_rate=lr)"
      ],
      "metadata": {
        "id": "dQ0cIE6m6g-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "num_classes = 10  # Set this to the number of categories you have\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # LSTM(128, input_shape=(max_length, 2)),  # 2 for x and y coordinates\n",
        "    # Dropout(0.2),\n",
        "    # Dense(64, activation='relu'),\n",
        "    # Dense(num_classes, activation='softmax')\n",
        "    Rescaling(1. / 255, input_shape=(64, 64, 1)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(6, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    Conv2D(8, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    Conv2D(10, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(700, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(500, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(400, activation=\"relu\"),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(len(categories), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=loss_fn, optimizer=optm, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Jd_fCuAKyNsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compiling the model**"
      ],
      "metadata": {
        "id": "lzWmFA-XHzUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TRNoCRS2Ht1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the model**"
      ],
      "metadata": {
        "id": "uAmWY6u1IXXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 32\n",
        "\n",
        "model_3.fit(\n",
        "    train_dataset,\n",
        "    validation_data = validation_dataset,\n",
        "    epochs = epochs\n",
        ")\n",
        "\n",
        "model_3.save(\"./models/test_model\")"
      ],
      "metadata": {
        "id": "DCKCsxhNIW_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # count the correct predictions\n",
        "    correct = 0\n",
        "    # when training, we put the model in train mode\n",
        "    model.train()\n",
        "    # we iterate over the dataloader\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # in each iteration, we work with a batch of 64 images\n",
        "        # we move to GPU first if GPU is available\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # step1: forward pass\n",
        "        ### Your code here (1 line) ###\n",
        "        pred = model(X)\n",
        "\n",
        "        ######################\n",
        "\n",
        "        # step2: compute prediction error/loss\n",
        "        ### Your code here (1 line) ###\n",
        "        loss = loss_fn(pred, y)\n",
        "        ######################\n",
        "\n",
        "        # counting the correct predictions\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        # step 3: Backpropagation & parameter updating\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        ######################\n",
        "\n",
        "        # step 4: zero the accumulated gradients in the tensors (1 lines)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        ######################\n",
        "\n",
        "\n",
        "        # every 100 batches, we print out the loss information to get an idea of\n",
        "        # how good we are\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"Running loss for a batch of 100 images: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    correct /= size\n",
        "    print(f\"Training accuracy for this epoch: {(100*correct):>0.1f}%\")"
      ],
      "metadata": {
        "id": "Wa6_YCnS2QyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "batch_size = 64\n",
        "model.fit(train_dataloader,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=test_dataloader)"
      ],
      "metadata": {
        "id": "Cl4UUdeDyXFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing accuracy**"
      ],
      "metadata": {
        "id": "daonDlfJIpNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model_3.evaluate(validation_dataset, verbose=2)"
      ],
      "metadata": {
        "id": "HbwD7Hr8I5lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input a random image**"
      ],
      "metadata": {
        "id": "JKpRw0MnJQNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_dataset.take(1):\n",
        "  data = images[0].numpy().astype(\"uint8\")\n",
        "  plt.imshow(data, cmap='gray', vmin=0, vmax=255)\n",
        "  plt.title(train_dataset.class_names[labels[0]])\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "9V6aBUCBJL6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make a prediction**"
      ],
      "metadata": {
        "id": "OAJTpvM9JuB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_data = train_dataset.take(1)\n",
        "for images, labels in prediction_data:\n",
        "  data = images[0].numpy().astype(\"uint8\")\n",
        "  plt.imshow(data, cmap='gray', vmin=0, vmax=255)\n",
        "  plt.title(train_dataset.class_names[labels[0]])\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "\n",
        "predictions = model_3.predict(prediction_data)\n",
        "categories[np.argmax(predictions[0])]"
      ],
      "metadata": {
        "id": "TGsUd6F4J1aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # when evaluating, we put the model in evaluation mode\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    # when evaluating, we disable gradient accumulations\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # this compute the accuracy\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "3Hetu2ie37nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "id": "AGRIC4jMyaJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "# save the state_dict to the path \"model.pth\"\n",
        "# TODO\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "id": "NLa8z27g4Frw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_dataloader, model, loss_fn)"
      ],
      "metadata": {
        "id": "f_b4PR8Z4V0S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}